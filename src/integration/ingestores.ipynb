{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b17ab8-089d-462b-8a22-69e448ce4889",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Classe para Ingestão de csv"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import delta\n",
    "class importfromcsv:\n",
    "    def __init__(self, config):\n",
    "        # Desempacota os parâmetros do dicionário para os atributos da classe\n",
    "        self.catalog = config.get(\"catalog\")\n",
    "        self.schema = config.get(\"schema\")\n",
    "        self.volume = config.get(\"volume\")\n",
    "        self.data_format = config.get(\"data_format\")\n",
    "        self.dest_catalog = config.get(\"dest_catalog\")\n",
    "        self.dest_format = config.get(\"dest_format\")\n",
    "        self.dest_schema = config.get(\"dest_schema\")\n",
    "        # Verifica se algum valor está ausente\n",
    "        if not all([self.catalog, self.schema, self.volume, self.data_format, self.dest_catalog, self.dest_format, self.dest_schema]):\n",
    "            raise ValueError(\"Faltam parâmetros necessários no dicionário de configuração.\")\n",
    "    def limpa_caracteres(self, col):\n",
    "        # Limpa caracteres especiais, substituindo-os por underscores\n",
    "        return re.sub(r'[:;,\\{\\}\\(\\)\\n\\t=\\s]', '_', col)\n",
    "    #Define os arquivos no diretorio\n",
    "    def set_files(self):\n",
    "        # Lista os arquivos do diretório específico no DBFS (Databricks File System)\n",
    "        files = dbutils.fs.ls(f\"/Volumes/{self.catalog}/{self.schema}/{self.volume}/\")\n",
    "        return files\n",
    "    \n",
    "    #Carrega arquivos\n",
    "    def load_data(self, nm_arquivo):\n",
    "        # Carrega o arquivo especificado utilizando o formato de dados desejado\n",
    "        file_path = f\"/Volumes/{self.catalog}/{self.schema}/{self.volume}/{nm_arquivo}\"\n",
    "        # Tente carregar o arquivo com o formato especificado\n",
    "        df = (spark.read\n",
    "                   .format(self.data_format)\n",
    "                   .option(\"header\", \"true\")\n",
    "                   .load(file_path))\n",
    "        return df\n",
    "    \n",
    "    #Salva arquivos no formato e diretorio de destino\n",
    "    def save_data(self, df, nm_arquivo):\n",
    "        # Salva o DataFrame no formato e diretório de destino\n",
    "        df.write.format(self.dest_format).mode(\"overwrite\").save(f\"/Volumes/{self.dest_catalog}/{self.dest_schema}/{nm_arquivo}\")\n",
    "    \n",
    "    #limpa diretorios de log do spark    \n",
    "    def clear_log(self,nm_arquivo):\n",
    "        caminho_destino = f\"/Volumes/{self.dest_catalog}/{self.dest_schema}/{nm_arquivo}/\"\n",
    "        arquivos_temp = dbutils.fs.ls(caminho_destino)\n",
    "        # Filtra e exclui arquivos que começam com _started_ ou _committed_\n",
    "        for arquivo in arquivos_temp:\n",
    "            if arquivo.name.startswith(\"_started_\") or arquivo.name.startswith(\"_committed_\") or arquivo.name.startswith(\"_SUCCESS\")        :\n",
    "                dbutils.fs.rm(arquivo.path)\n",
    "    #Executa\n",
    "    def execute(self):\n",
    "        # Executa a ingestão dos arquivos no diretório configurado\n",
    "        files = self.set_files()  # Obtém a lista de arquivos\n",
    "        for file in files:\n",
    "            nm_arquivo = file.name  # Nome do arquivo\n",
    "            df = self.load_data(nm_arquivo)  # Carrega o arquivo\n",
    "\n",
    "            # Limpa os nomes das colunas\n",
    "            nm_nova_colunas = [self.limpa_caracteres(col) for col in df.columns]\n",
    "            df_colunas = df.toDF(*nm_nova_colunas)  # Renomeia as colunas\n",
    "\n",
    "            # Remove a extensão '.csv' do nome do arquivo\n",
    "            nm_arquivo = nm_arquivo.replace(\".csv\", \"\")\n",
    "\n",
    "            # Salva os dados processados\n",
    "            self.save_data(df_colunas, nm_arquivo)\n",
    "            #limpa diretorio de log do spark\n",
    "            self.clear_log(nm_arquivo)\n",
    "\n",
    "            #print(f\"Arquivo {nm_arquivo} processado com sucesso!\")\n",
    "\n",
    "class importfromraw:\n",
    "    def __init__(self, config):\n",
    "        # Desempacota os parâmetros do dicionário para os atributos da classe\n",
    "        self.catalog = config.get(\"catalog\")\n",
    "        self.schema = config.get(\"schema\")\n",
    "        #self.volume = config.get(\"volume\")\n",
    "        self.data_format = config.get(\"data_format\")\n",
    "        self.dest_catalog = config.get(\"dest_catalog\")\n",
    "        self.dest_format = config.get(\"dest_format\")\n",
    "        self.dest_schema = config.get(\"dest_schema\")\n",
    "        self.ref_path = config.get(\"ref_path\")\n",
    "    \n",
    "    def set_files(self):\n",
    "        # Lista os arquivos do diretório\n",
    "        files = dbutils.fs.ls(self.ref_path)\n",
    "        return files\n",
    "    \n",
    "    def load_data(self, nm_arquivo):\n",
    "        file_path = f\"/Volumes/{self.catalog}/{self.schema}/{nm_arquivo}\"\n",
    "        df = spark.read.format(self.data_format).load(file_path)\n",
    "        return df\n",
    "    def save_data(self, df, nm_arquivo):\n",
    "        df.write.format(self.dest_format).mode(\"overwrite\").saveAsTable(f\"{self.dest_catalog}.{self.dest_schema}.{nm_arquivo}\")\n",
    "    #Executa\n",
    "    def execute(self):\n",
    "        files = self.set_files()  # Obtém a lista de arquivos\n",
    "        for file in files:\n",
    "            #remove a extensão original do arquivo\n",
    "            nm_arquivo = file.name.replace(\".csv\",\"\")\n",
    "            #carrega o arquivo no formato parquet\n",
    "            df = self.load_data(nm_arquivo)  # Carrega o arquivo\n",
    "            #Salva o arquivo no formato delta na camada bronze\n",
    "            self.save_data(df,nm_arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346d88b3-fc84-4c34-b778-b6ce4bb0d4ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execução"
    }
   },
   "outputs": [],
   "source": [
    "##exemplo de config importfromcsv\n",
    "#config = {\n",
    "#    \"catalog\": \"data_source\",\n",
    "#    \"schema\": \"data_csv\",\n",
    "#    \"volume\": \"tables_csv\",\n",
    "#    \"data_format\": \"csv\",\n",
    "#    \"dest_catalog\": \"raw\",\n",
    "#    \"dest_format\": \"parquet\",\n",
    "#    \"dest_schema\": \"vendas\"\n",
    "#}\n",
    "#\n",
    "#importfromcsv(config).execute()\n",
    "#\n",
    "#\n",
    "##exemplo de config importfromraw\n",
    "#config = {\n",
    "#    \"catalog\": \"raw\",\n",
    "#    \"schema\": \"vendas\",\n",
    "#    \"data_format\": \"parquet\",\n",
    "#    \"dest_catalog\": \"bronze\",\n",
    "#    \"dest_format\": \"delta\",\n",
    "#    \"dest_schema\": \"vendas\",\n",
    "#    \"ref_path\":\"/Volumes/data_source/data_csv/tables_csv/\"\n",
    "#}\n",
    "#\n",
    "#importfromraw(config).execute()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4327277038016554,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestores",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
